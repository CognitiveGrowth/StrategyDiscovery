% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 
% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
%\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{units}
\usepackage{textcomp}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\setlength{\tabcolsep}{1.5pt}

\title{Towards a rational theory of meta-decision making}
\author{TBD}
%{\large \bf Falk Lieder (falk.lieder@berkeley.edu)} \\
%  Helen Wills Neuroscience Institute, University of California at Berkeley, CA, USA
%  \AND{\large \bf Paul M. Krueger (pmk@berkeley.edu)} \\
%  Department of Psychology, University of California at Berkeley, CA, USA
%  \AND{\large \bf Thomas L. Griffiths (tom\_griffiths@berkeley.edu)} \\
%  Department of Psychology, University of California at Berkeley, CA, USA}

\begin{document}

\maketitle

\begin{abstract}
%Introduction
%Methods
%Results
%Conclusion

\textbf{Keywords:} 
Decision-Making;  Heuristics; Meta Decision-Making
\end{abstract}

\section{Introduction}
%big picture: decision strategies, heuristics, resource-rationality, adaptive flexibility
\paragraph{big picture}: decision strategies, heuristics, adaptive flexibility, and the debate about human rationality

%general approach:  meta-level MDPs and optimal meta-level policies
\paragraph{our approach: } resource-rationality, rational meta-decision making as the optimal solution to a meta-level MDP

%payoffs: what do we gain from this?
\paragraph{payoffs: }
\begin{enumerate}
\item{a better normative standard of rational decision making that takes into account that people's time is finite and that their computational resources are bounded}
\item{an automatic way to discover novel decision strategies}
\item{new insights into how people make decisions under limited resources}
\item{an alternative to toolbox theories of judgment and decision making}
\item{a fairer judgment of human rationality}
\end{enumerate} 

%our specific contribution
\paragraph{our specific contribution: } a resource-rational model of decision-making in the Mouselab paradigm, empirical test of novel predictions, discovery of a new decision strategy

%plan for the paper
\paragraph{plan for th paper: } 

\section{Markov Decision Processes}
Each sequential decision problem can be modeled as a \textit{Markov Decision Process} (MDP)
\begin{equation}
	M=\left( \mathcal{S}, \mathcal{A}, T, \gamma, r, P_0 \right),
\end{equation}
where $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $T(s,a,s')$ is the probability that the agent will transition from state $s$ to state $s'$ if it takes action $a$, $0\leq \gamma \leq 1$ is the discount factor, $r(s,a,s')$ is the reward generated by this transition, and $P_0$ is the probability distribution of the initial state $S_0$ \cite{Sutton1998}. A \textit{policy} $\pi: \mathcal{S} \mapsto \mathcal{A}$ specifies which action to take in each of the states. The expected sum of discounted rewards that a policy $\pi$ will generate in the MDP $M$ starting from a state $s$ is known as its \textit{value function}
\begin{equation} \label{eq:ValueFunction}
	V_M^\pi(s)=\mathds{E}\left[ \sum_{t=0}^\infty {\gamma^{t} \cdot r\left(S_t,\pi(S_t),S_{t+1}\right)} \right].
\end{equation} 
The optimal policy $\pi_M^\star$ maximizes the expected sum of discounted rewards, that is
\begin{equation}
\pi_M^\star = \arg\max_\pi \mathds{E}\left[ \sum_{t=0}^\infty {\gamma^{t} \cdot r\left(S_t,\pi(S_t),S_{t+1}\right)} \right],
\end{equation}

\section{Deciding how to decide}

\subsection{The adaptive decision-maker}
\begin{enumerate}
\item{The Mouselab paradigm: many alternative decision strategies}
\item{information acquisitions as a window on the decision process}
\end{enumerate}

\subsection{Optimal meta-decision-making}
\begin{enumerate}
\item{Meta-level MDPs as a computational-level theory of deciding how to decide \cite{Hay2012}.}
\item{Meta-level MDP of the Mouselab task}
\item{Approximating the optimal meta-level policy: Bayesian value function approximation}
\end{enumerate}


%\item{meta-level MDPs and optimal pseudo-rewards as incentives for good decision strategies}

\section{Experimental Test of novel predictions}
\subsection{Model Predictions}
\begin{enumerate}
\item{emergence of familiar decision strategies like TTB and WADD for specific problems}
\item{ problem-contingent ``strategy selection'' including the effect of compensatoriness}
\item{previously unobserved effects of people's prior knowledge about the distribution of possible payoffs on their decision process}
\item{Previously unobserved SAT-TTB hybrid strategy terminates decision process early when a high payoff is observed on a probable outcome and the range of payoffs is small compared to the cost of time}
\item{Information acquisition become systematically more frugal as the range of possible payoffs decreases}
\end{enumerate}

\subsection{Methods}
\paragraph{Participants:} We will recruit 200 participants on Amazon Mechanical Turk. Based on \citeA{Payne1988}, we expect the task to take about 30 minutes. Participants will receive a baseline payment of $\$1.50$ to guarantee a minimum rate of $\$3$ per hour, and can earn a bonus of up to $\$9.99$.

\paragraph{Procedure:}
Mouselab experiment with
\begin{itemize}\item{2 blocks a 20 trials with 4 outcomes}\item{inspected outcomes remain visible on the screen}\item{no time limit}\item{participants receive the payoff from a randomly selected trial}
\end{itemize}\paragraph{Experimental Design:}
2x2x2 within subjects design:
\begin{enumerate}\item{IV1: range of payoffs: manipulated within subjects across blocks Ð either $[\$0.00; \$0.25]$ vs. $[\$0.01; \$9.99]$}.\item{IV2: number of gambles: 2 vs. 7; 5 instances of each in each block}\item{IV3: compensatoriness: highly non-compensatory (e.g., $[0.9, 0.05, 0.03, 0.02]$) vs. nearly uniform (e.g., $[0.35,0.25,0.2,0.15]$); 5 instances of each in each block}
\end{enumerate}

\subsection{Results}

\section{Discussion}
\begin{enumerate}
\item{summary, implications, and future directions}
\item{conclusion}
\end{enumerate}

\vspace{2mm}
\begin{small}
\noindent {\bf Acknowledgments.}
This work was supported by grant number ONR MURI N00014-13-1-0341.
\end{small}

\bibliographystyle{apacite}

%\setlength{\bibleftmargin}{.125in}
%\setlength{\bibindent}{-\bibleftmargin}

\bibliographystyle{apacite}
\renewcommand{\bibliographytypesize}{\small}
\setlength{\bibleftmargin}{.05in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{gamification,flieder-algorithm-selection.big}


\end{document}
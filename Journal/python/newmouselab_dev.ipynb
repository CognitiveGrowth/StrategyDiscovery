{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, dirichlet\n",
    "import scipy.integrate as integrate\n",
    "import gym\n",
    "import random\n",
    "from functools import lru_cache\n",
    "from agents import Agent\n",
    "from evaluation import *\n",
    "from distributions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Random Variable\n",
    "\n",
    "This is an abstraction that allows us to interact with the multivariate random variable that will be used to represent the distribution. We will need a way to get the expected distribution, sample or observe individual elements, and sample or observe all elements at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRV(object):\n",
    "    \"\"\"An object that \"\"\"\n",
    "    def __init__(self, alpha, attributes, ground_truth = None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.attributes = attributes\n",
    "        self.num_unobs = attributes\n",
    "        self.state = np.ones(attributes)*-1\n",
    "        \n",
    "        if ground_truth is False:\n",
    "            self.ground_truth = False\n",
    "        elif ground_truth is not None:\n",
    "            self.ground_truth = np.array(ground_truth)\n",
    "        else:\n",
    "            self.ground_truth = self.sample_all()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'DistRV(a=' + str(self.alpha) + '): [' + \", \".join(self.print_dist()) + \"]\" \n",
    "    \n",
    "    def print_dist(self):\n",
    "        return ['{:.3f}'.format(self.state[i]) if self.state[i] != -1 else 'p' +str(i) for i in range(self.attributes)]\n",
    "    \n",
    "    def observe_p(self, i):\n",
    "        if self.state[i] == -1:\n",
    "            self.num_unobs -= 1\n",
    "            if self.ground_truth is not False:\n",
    "                self.state[i] = self.ground_truth[i]\n",
    "            else:\n",
    "                self.state[i] = np.random.beta(self.alpha, self.alpha*(self.num_unobs - 1))\n",
    "        return self.state\n",
    "    \n",
    "    def sample_p(self, i, n=1, expectation = False):\n",
    "        p_vec = np.repeat(self.state[None,:],n,axis=0)\n",
    "        if self.num_unobs == 0:\n",
    "            return p_vec\n",
    "        if self.state[i] == -1:\n",
    "            if self.num_unobs == 1:\n",
    "                p_vec[:,i] = (1-np.sum(p_vec[:,p_vec[0] != -1],1))\n",
    "            else:\n",
    "                p_vec[:,i] = np.random.beta(self.alpha, self.alpha*(self.num_unobs - 1),size=n)\n",
    "        if expectation:\n",
    "            filler = (1-np.sum(p_vec[:,p_vec[0] != -1],1)[:,None])\n",
    "            if self.num_unobs > 1:\n",
    "                filler /= (self.num_unobs - 1)\n",
    "            p_vec[:,p_vec[0] == -1] = filler\n",
    "        return np.squeeze(p_vec)\n",
    "    \n",
    "    def sample_all(self, n=1):\n",
    "        p_vec = np.repeat(self.state[None,:],n,axis=0)\n",
    "        if self.num_unobs == 0:\n",
    "            return p_vec\n",
    "        else:\n",
    "            alpha_vec = np.ones(self.num_unobs)*self.alpha\n",
    "            sampled_unobs = np.random.dirichlet(alpha_vec,size=n)\n",
    "            p_vec[:,self.state == -1] = (1-np.sum(self.state[self.state != -1])) * sampled_unobs\n",
    "        return np.squeeze(p_vec)\n",
    "    \n",
    "    def expectation(self):\n",
    "        p_vec = np.copy(self.state)\n",
    "        if self.num_unobs != 0:\n",
    "            p_vec[p_vec == -1] = (1-np.sum(p_vec[p_vec != -1]))/self.num_unobs\n",
    "        return p_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DistRV(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.122,  0.129,  0.335,  0.225,  0.189]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.dirichlet([3,3,3,3,3],size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(a.state,1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistRV(a=5): [p0, p1, p2, p3, p4]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.177,  0.244,  0.169,  0.181,  0.229])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2,  0.2,  0.2,  0.2,  0.2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.expectation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.   , -1.   ,  0.295, -1.   , -1.   ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sample_p(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistRV(a=5): [p0, p1, p2, p3, p4]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2,  0.2,  0.2,  0.2,  0.2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.expectation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.107,  0.235,  0.118,  0.337,  0.202])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sample_all(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.197,  0.197,  0.197,  0.213,  0.197],\n",
       "       [ 0.197,  0.197,  0.197,  0.21 ,  0.197],\n",
       "       [ 0.215,  0.215,  0.215,  0.14 ,  0.215],\n",
       "       ..., \n",
       "       [ 0.174,  0.174,  0.174,  0.305,  0.174],\n",
       "       [ 0.201,  0.201,  0.201,  0.196,  0.201],\n",
       "       [ 0.234,  0.234,  0.234,  0.066,  0.234]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sample_p(3,expectation=True,n=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.num_unobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.177,  0.244,  0.169,  0.181,  0.229])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.177,  0.244,  0.169,  0.181, -1.   ])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.observe_p(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.068,  0.251,  0.192,  0.277,  0.212])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sample_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistRV(a=5): [p0, 0.293, 0.189, 0.125, 0.127]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.177,  0.244,  0.169,  0.181,  0.229],\n",
       "       [ 0.177,  0.244,  0.169,  0.181,  0.229],\n",
       "       [ 0.177,  0.244,  0.169,  0.181,  0.229],\n",
       "       ..., \n",
       "       [ 0.177,  0.244,  0.169,  0.181,  0.229],\n",
       "       [ 0.177,  0.244,  0.169,  0.181,  0.229],\n",
       "       [ 0.177,  0.244,  0.169,  0.181,  0.229]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sample_p(4,10000000,expectation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.177,  0.244,  0.169,  0.181,  0.229])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.expectation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ZERO = PointMass(0)\n",
    "\n",
    "class NewMouselabEnv(gym.Env):\n",
    "    \"\"\"MetaMDP for the Mouselab task.\"\"\"\n",
    "\n",
    "    term_state = '__term_state__'\n",
    "    def __init__(self, gambles=4, attributes=5, reward=None, cost=0,\n",
    "                 ground_truth=None, alpha=1, sample_term_reward=False, quantization=False):\n",
    "\n",
    "        self.gambles = gambles # number of gambles\n",
    "        self.quantization = quantization\n",
    "        self.attributes = attributes\n",
    "        self.outcomes = attributes\n",
    "\n",
    "        self.distRV = DistRV(alpha, attributes, ground_truth = ground_truth)\n",
    "        self.reward = reward if reward is not None else Normal(1., 1.)\n",
    "        \n",
    "        if quantization:\n",
    "            self.discrete_reward = self.reward.to_discrete(quantization)\n",
    "\n",
    "        if hasattr(reward, 'sample'):\n",
    "            self.iid_rewards = True\n",
    "        else:\n",
    "            self.iid_rewards = False\n",
    "\n",
    "        self.cost = - abs(cost)\n",
    "        self.max = cmax\n",
    "        self.init_rewards = tuple([self.reward,] * (self.gambles*self.outcomes))\n",
    "        self.init = (self.distRV, self.init_rewards)\n",
    "\n",
    "        # self.ground_truth only includes rewards\n",
    "        # self.distRV.ground_truth has the distribution ground truth\n",
    "        if ground_truth is False:\n",
    "            self.ground_truth = False\n",
    "        elif ground_truth is not None:\n",
    "            self.ground_truth = np.array(ground_truth)\n",
    "        else:\n",
    "            if self.quantization:\n",
    "                self.ground_truth = np.array([self.discrete_reward.sample() for _ in self.init])\n",
    "            else:\n",
    "                self.ground_truth = np.array(list(map(sample, self.init[1])))\n",
    "\n",
    "        self.sample_term_reward = sample_term_reward\n",
    "        self.term_action = (self.gambles+1)*self.outcomes\n",
    "        self.reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = self.init\n",
    "        grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "        self.dist = self.distRV.expectation()\n",
    "        # tmp: Works only for Normal, possibly generalizable format:\n",
    "        # self.mus = [expectation(np.sum(self.dist*grid[g])) for g in range(self.gambles)]\n",
    "        # self.vars = [(np.sum(self.dist*grid[g])).sigma**2 for g in range(self.gambles)]\n",
    "        self.mus = self.reward.mu*np.ones(self.gambles)\n",
    "        self.vars = np.sum(self.dist**2*self.reward.sigma**2)*np.ones(self.gambles)\n",
    "        return self._state\n",
    "\n",
    "    def _step(self, action):\n",
    "        self.vpi.cache_clear()\n",
    "        self.vpi_action.cache_clear()\n",
    "        if self._state is self.term_state:\n",
    "            assert 0, 'state is terminal'\n",
    "            # return None, 0, True, {}\n",
    "        if action >= self.term_action:\n",
    "            # self._state = self.term_state\n",
    "            if self.sample_term_reward:\n",
    "                if self.ground_truth is not False:\n",
    "                    best_idx = np.argmax(self.mus)\n",
    "                    gt_grid = self.ground_truth.reshape(self.gambles,self.outcomes)\n",
    "                    reward = self.dist.dot(gt_grid[best_idx])\n",
    "                else:\n",
    "                    reward = sample(self.term_reward())\n",
    "            else:\n",
    "                reward = self.expected_term_reward()\n",
    "            self.last_state = self._state\n",
    "            self._state = self.term_state\n",
    "            done = True\n",
    "        elif self.term_action > action >= self.attributes:\n",
    "            if not hasattr(self._state[1][action-self.attributes], 'sample'):  # already observed reward\n",
    "    #             assert 0, self._state[action]\n",
    "                reward = 0      \n",
    "            else:  # observe a new node\n",
    "                self._state = self._observe(action)\n",
    "                reward = self.cost\n",
    "            done = False\n",
    "        else:\n",
    "            if not self._state[0].state[action] == -1: # already observed attribute\n",
    "                reward = 0\n",
    "            else:  # observe a new attribute\n",
    "                self._state = self._observe(action)\n",
    "                reward = self.cost #todo: possibly have a separate cost for p observations\n",
    "            done = False\n",
    "        return self._state, reward, done, {}\n",
    "\n",
    "    def _observe(self, action):\n",
    "#         print('obs ' + str(action))\n",
    "        if action >= self.attributes:\n",
    "            action -= self.attributes\n",
    "            if self.ground_truth is not False:\n",
    "                result = self.ground_truth[action]\n",
    "            elif self.quantization:\n",
    "                assert hasattr(self._state[action], 'sample')\n",
    "                result = self.discrete_reward.sample()\n",
    "            else:\n",
    "                result = self._state[action].sample()\n",
    "            s = list(self._state[1])\n",
    "            gamble = action // self.outcomes\n",
    "            option = action % self.outcomes\n",
    "            self.mus[gamble] += self.dist[option]*(result - self.reward.expectation())\n",
    "            self.vars[gamble] = max(0,self.vars[gamble] - self.dist[option]**2*self.reward.sigma**2)\n",
    "            s[action] = result\n",
    "            return (self._state[0],tuple(s))\n",
    "        else:\n",
    "            # edit so it is a temporary change unless assigned\n",
    "            self._state[0].observe_p(action)\n",
    "            self.dist = self._state[0].expectation()\n",
    "            return self._state\n",
    "\n",
    "    def actions(self, state=None):\n",
    "        \"\"\"Yields actions that can be taken in the given state.\n",
    "\n",
    "        Actions include observing the value of each unobserved node and terminating.\n",
    "        \"\"\"\n",
    "        rewards = state if state is not None else self._state[1]\n",
    "        if state is self.term_state:\n",
    "            return\n",
    "        for i in range(self.attributes):\n",
    "            if self._state[0].state[i] == -1:\n",
    "                yield i\n",
    "        for i, v in enumerate(rewards):\n",
    "            if hasattr(v, 'sample'):\n",
    "                yield i + self.attributes\n",
    "        yield self.term_action\n",
    "\n",
    "    #todo: update\n",
    "    def results(self, state, action):\n",
    "        \"\"\"Returns a list of possible results of taking action in state.\n",
    "\n",
    "        Each outcome is (probability, next_state, reward).\n",
    "        \"\"\"\n",
    "        # May not work with p random variables (at least without quantization)\n",
    "        if action == self.term_action:\n",
    "            # R = self.term_reward()\n",
    "            # S1 = Categorical([self.term_state])\n",
    "            # return cross(S1, R)\n",
    "            yield (1, self.term_state, self.expected_term_reward(state))\n",
    "        else:\n",
    "            for r, p in state[action].to_discrete(self.quantization):\n",
    "                s1 = list(state[1])\n",
    "                s1[action] = r\n",
    "                yield (p, tuple(s1), self.cost)\n",
    "\n",
    "\n",
    "    def action_features(self, action, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        assert state is not None\n",
    "\n",
    "\n",
    "        if action == self.term_action:\n",
    "            return np.array([\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                self.expected_term_reward(state)\n",
    "            ])\n",
    "        else:\n",
    "            gamble = action // self.outcomes\n",
    "            gamble = -action if gamble == 0 else gamble\n",
    "            return np.array([\n",
    "                self.cost,\n",
    "                self.myopic_voi(action),\n",
    "                self.vpi_action(gamble),\n",
    "                self.vpi(),\n",
    "                self.expected_term_reward(state)\n",
    "            ])\n",
    "\n",
    "    def gamble_dists(self, state, sample_all = False):\n",
    "        sdist = state[0].sample_all() if sample_all else self.dist\n",
    "        grid = np.array(state[1]).reshape(self.gambles, self.outcomes)\n",
    "        return np.dot(grid, sdist)\n",
    "    \n",
    "    def print_state(self,state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        if state is self.term_state:\n",
    "            return self.print_state(state = self.final_state)\n",
    "        return pd.DataFrame(a.grid(),columns=state[0].print_dist())\n",
    "    \n",
    "    def grid(self,state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        if state is self.term_state:\n",
    "            return np.array(self.last_state).reshape(self.gambles,self.outcomes)\n",
    "        return np.array(state[1]).reshape(self.gambles,self.outcomes)\n",
    "\n",
    "    @lru_cache(None)\n",
    "    def vpi(self):\n",
    "        sdist = self._state[0].sample_all(2500)\n",
    "        grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "        sampled_gambles = np.vectorize(lambda g: sample(g))(sdist.dot(grid.T))\n",
    "        samples_max = np.amax(sampled_gambles,1)\n",
    "        return np.mean(samples_max) - np.max(self.mus)\n",
    "\n",
    "    @lru_cache(None)\n",
    "    def vpi_action(self, gamble):\n",
    "        #E[value if gamble corresponding to action is fully known]\n",
    "        if gamble > 0:\n",
    "            gamble -= 1\n",
    "            mus_wo_g = np.delete(self.mus,gamble)\n",
    "            k = np.max(mus_wo_g)\n",
    "            m = self.mus[gamble]\n",
    "            s = np.sqrt(self.vars[gamble])\n",
    "            e_higher = integrate.quad(lambda x: x*norm.pdf(x,m,s), k, np.inf)[0]\n",
    "            e_val = k*norm.cdf(k,m,s) + e_higher\n",
    "        else:\n",
    "            action = -1*gamble\n",
    "            sdist = self._state[0].sample_all(2500)\n",
    "            grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "            rgrid = np.repeat(grid[None,:,:],2500,axis=0)\n",
    "            rgrid[:,:,action] = np.vectorize(lambda g: sample(g))(rgrid[:,:,action])\n",
    "            rgrid = np.vectorize(lambda g: expectation(g))(rgrid)\n",
    "            sampled_gambles = np.einsum('ijk,ik->ij',rgrid,sdist)\n",
    "            e_val = np.mean(np.amax(sampled_gambles,1))\n",
    "        return e_val - np.max(self.mus)\n",
    "\n",
    "    def myopic_voi(self, action):\n",
    "        #E[value if gamble corresponding to action is fully known]\n",
    "        if action >= self.attributes:\n",
    "            action -= self.attributes\n",
    "            gamble = action // self.outcomes\n",
    "            outcome = action % self.outcomes\n",
    "            mus_wo_g = np.delete(self.mus,gamble)\n",
    "            k = np.max(mus_wo_g)\n",
    "            m = self.mus[gamble]\n",
    "            s = self.reward.sigma*self.dist[outcome]\n",
    "            e_higher = integrate.quad(lambda x: x*norm.pdf(x,m,s), k, np.inf)[0]\n",
    "            e_val = k*norm.cdf(k,m,s) + e_higher\n",
    "        else:\n",
    "            grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "            sdist = self._state[0].sample_p(action, n = 2500, expectation = True)\n",
    "            smus = np.vectorize(lambda g: expectation(g))(sdist.dot(grid.T))\n",
    "            e_val = np.mean(np.amax(smus,1))\n",
    "        return e_val - np.max(self.mus)\n",
    "\n",
    "    def term_reward(self, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        grid = np.array(state[1]).reshape(self.gambles,self.outcomes)\n",
    "        best_idx = np.argmax(self.mus)\n",
    "        return self.dist.dot(grid[best_idx])\n",
    "\n",
    "    def expected_term_reward(self, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        return max(map(expectation, self.gamble_dists(state)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = NewMouselabEnv(4,5,alpha=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((DistRV(a=3): [p0, p1, 0.148, p3, 0.174],\n",
       "  (Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   1.2432675989466333,\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   0.73861249545240182,\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00),\n",
       "   Norm(1.00, 1.00))),\n",
       " 0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.step(2)\n",
    "a.step(12)\n",
    "a.step(4)\n",
    "a.step(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>0.148</th>\n",
       "      <th>p3</th>\n",
       "      <th>0.174</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>1.24327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>0.738612</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "      <td>Norm(1.00, 1.00)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 p0                p1             0.148                p3  \\\n",
       "0  Norm(1.00, 1.00)  Norm(1.00, 1.00)  Norm(1.00, 1.00)  Norm(1.00, 1.00)   \n",
       "1  Norm(1.00, 1.00)  Norm(1.00, 1.00)          0.738612  Norm(1.00, 1.00)   \n",
       "2  Norm(1.00, 1.00)  Norm(1.00, 1.00)  Norm(1.00, 1.00)  Norm(1.00, 1.00)   \n",
       "3  Norm(1.00, 1.00)  Norm(1.00, 1.00)  Norm(1.00, 1.00)  Norm(1.00, 1.00)   \n",
       "\n",
       "              0.174  \n",
       "0           1.24327  \n",
       "1  Norm(1.00, 1.00)  \n",
       "2  Norm(1.00, 1.00)  \n",
       "3  Norm(1.00, 1.00)  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRV(object):\n",
    "    \"\"\"An object that \"\"\"\n",
    "    def __init__(self, alpha, attributes, ground_truth = None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.attributes = attributes\n",
    "        self.num_unobs = attributes\n",
    "        self.state = np.ones(attributes)*-1\n",
    "        if ground_truth is False:\n",
    "            self.ground_truth = False\n",
    "        elif ground_truth is not None:\n",
    "            self.ground_truth = np.array(ground_truth)\n",
    "        else:\n",
    "            self.ground_truth = False\n",
    "            tmp = self.sample_all()\n",
    "            self.ground_truth = tmp\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'DistRV(a=' + str(self.alpha) + '): [' + \", \".join(self.print_dist()) + \"]\" \n",
    "    \n",
    "    def print_dist(self):\n",
    "        return ['{:.3f}'.format(self.state[i]) if self.state[i] != -1 else 'p' +str(i) for i in range(self.attributes)]\n",
    "    \n",
    "    def sample_p(self, i, observe = False, expectation = False):\n",
    "        p_vec = self.state if observe else np.copy(self.state)\n",
    "        if p_vec[i] == -1:\n",
    "            p_vec[i] = np.random.beta(self.alpha, self.alpha*(self.num_unobs - 1))\n",
    "            if observe:\n",
    "                self.num_unobs -= 1\n",
    "                if self.ground_truth is not False:\n",
    "                    p_vec[i] = self.ground_truth[i]\n",
    "        if expectation:\n",
    "            p_vec[p_vec == -1] = (1-np.sum(p_vec[p_vec != -1]))/(self.num_unobs - 1)\n",
    "        return p_vec\n",
    "    \n",
    "    def sample_all(self, observe = False):\n",
    "        p_vec = self.state if observe else np.copy(self.state)\n",
    "        if self.num_unobs == 0:\n",
    "            return p_vec\n",
    "        else:\n",
    "            alpha_vec = np.ones(self.num_unobs)*self.alpha\n",
    "            sampled_unobs = np.random.dirichlet(alpha_vec,size=1)[0]\n",
    "            p_vec[self.state == -1] = (1-np.sum(p_vec[p_vec != -1])) * sampled_unobs\n",
    "        if observe:\n",
    "            self.num_unobs = 0\n",
    "            if self.ground_truth is not False:\n",
    "                p_vec = self.ground_truth\n",
    "        return p_vec\n",
    "    \n",
    "    def expectation(self):\n",
    "        p_vec = np.copy(self.state)\n",
    "        if self.num_unobs != 0:\n",
    "            p_vec[p_vec == -1] = (1-np.sum(p_vec[p_vec != -1]))/self.num_unobs\n",
    "        return p_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ZERO = PointMass(0)\n",
    "\n",
    "class OldMouselabEnv(gym.Env):\n",
    "    \"\"\"MetaMDP for the Mouselab task.\"\"\"\n",
    "\n",
    "    term_state = '__term_state__'\n",
    "    def __init__(self, gambles=4, attributes=5, reward=None, cost=0,\n",
    "                 ground_truth=None, alpha=1, sample_term_reward=False, quantization=False):\n",
    "\n",
    "        self.gambles = gambles # number of gambles\n",
    "        self.quantization = quantization\n",
    "        self.attributes = attributes\n",
    "        self.outcomes = attributes\n",
    "\n",
    "        self.distRV = DistRV(alpha, attributes, ground_truth = ground_truth)\n",
    "        self.reward = reward if reward is not None else Normal(1, 1)\n",
    "        \n",
    "        if quantization:\n",
    "            self.discrete_reward = self.reward.to_discrete(quantization)\n",
    "\n",
    "        if hasattr(reward, 'sample'):\n",
    "            self.iid_rewards = True\n",
    "        else:\n",
    "            self.iid_rewards = False\n",
    "\n",
    "        self.cost = - abs(cost)\n",
    "        self.max = cmax\n",
    "        self.init_rewards = tuple([self.reward,] * (self.gambles*self.outcomes))\n",
    "        self.init= (self.distRV, self.init_rewards)\n",
    "\n",
    "        if ground_truth is False:\n",
    "            self.ground_truth = False\n",
    "        elif ground_truth is not None:\n",
    "            self.ground_truth = np.array(ground_truth)\n",
    "        else:\n",
    "            if self.quantization:\n",
    "                self.ground_truth = np.array([self.discrete_reward.sample() for _ in self.init])\n",
    "            else:\n",
    "                self.ground_truth = np.array(list(map(sample, self.init[1])))\n",
    "\n",
    "        self.sample_term_reward = sample_term_reward\n",
    "        self.term_action = (self.gambles+1)*self.outcomes\n",
    "        self.reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = self.init\n",
    "        grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "        self.dist = self.distRV.expectation()\n",
    "        self.mus = [expectation(np.sum(self.dist*grid[g])) for g in range(self.gambles)]\n",
    "        # todo: include max_mu\n",
    "        # tmp: Works only for Normal\n",
    "        self.vars = np.sum(self.dist**2*self.reward.sigma**2)*np.ones(self.gambles)\n",
    "        return self._state\n",
    "\n",
    "    def _step(self, action):\n",
    "        self.vpi.cache_clear()\n",
    "        self.vpi_action.cache_clear()\n",
    "        if self._state is self.term_state:\n",
    "            assert 0, 'state is terminal'\n",
    "            # return None, 0, True, {}\n",
    "        if action >= self.term_action:\n",
    "            # self._state = self.term_state\n",
    "            if self.sample_term_reward:\n",
    "                if self.ground_truth is not False:\n",
    "                    best_idx = np.argmax(self.mus)\n",
    "                    gt_grid = self.ground_truth.reshape(self.gambles,self.outcomes)\n",
    "                    reward = self.dist.dot(gt_grid[best_idx])\n",
    "                else:\n",
    "                    reward = sample(self.term_reward())\n",
    "            else:\n",
    "                reward = self.expected_term_reward()\n",
    "            self.last_state = self._state\n",
    "            self._state = self.term_state\n",
    "            done = True\n",
    "        elif self.term_action > action >= self.attributes:\n",
    "            if not hasattr(self._state[1][action-self.attributes], 'sample'):  # already observed reward\n",
    "    #             assert 0, self._state[action]\n",
    "                reward = 0      \n",
    "            else:  # observe a new node\n",
    "                self._state = self._observe(action)\n",
    "                reward = self.cost\n",
    "            done = False\n",
    "        else:\n",
    "            if not self._state[0].state[action] == -1: # already observed attribute\n",
    "                reward = 0\n",
    "            else:  # observe a new attribute\n",
    "                self._state = self._observe(action)\n",
    "                reward = self.cost #todo: possibly have a separate cost for p observations\n",
    "            done = False\n",
    "        return self._state, reward, done, {}\n",
    "\n",
    "    def _observe(self, action):\n",
    "#         print('obs ' + str(action))\n",
    "        if action >= self.attributes:\n",
    "            action -= self.attributes\n",
    "            if self.ground_truth is not False:\n",
    "                result = self.ground_truth[action]\n",
    "            elif self.quantization:\n",
    "                assert hasattr(self._state[action], 'sample')\n",
    "                result = self.discrete_reward.sample()\n",
    "            else:\n",
    "                result = self._state[action].sample()\n",
    "            s = list(self._state[1])\n",
    "            gamble = action // self.outcomes\n",
    "            option = action % self.outcomes\n",
    "            self.mus[gamble] += self.dist[option]*(result - self.reward.expectation())\n",
    "            self.vars[gamble] = max(0,self.vars[gamble] - self.dist[option]**2*self.reward.sigma**2)\n",
    "            s[action] = result\n",
    "            return (self._state[0],tuple(s))\n",
    "        else:\n",
    "            # edit so it is a temporary change unless assigned\n",
    "            self._state[0].sample_p(action, True)\n",
    "            self.dist = self._state[0].expectation()\n",
    "            return self._state\n",
    "\n",
    "    def actions(self, state=None):\n",
    "        \"\"\"Yields actions that can be taken in the given state.\n",
    "\n",
    "        Actions include observing the value of each unobserved node and terminating.\n",
    "        \"\"\"\n",
    "        rewards = state if state is not None else self._state[1]\n",
    "        if state is self.term_state:\n",
    "            return\n",
    "        for i in range(self.attributes):\n",
    "            if self._state[0].state[i] == -1:\n",
    "                yield i\n",
    "        for i, v in enumerate(rewards):\n",
    "            if hasattr(v, 'sample'):\n",
    "                yield i + self.attributes\n",
    "        yield self.term_action\n",
    "\n",
    "    def results(self, state, action):\n",
    "        \"\"\"Returns a list of possible results of taking action in state.\n",
    "\n",
    "        Each outcome is (probability, next_state, reward).\n",
    "        \"\"\"\n",
    "        # May not work with p random variables (at least without quantization)\n",
    "        if action == self.term_action:\n",
    "            # R = self.term_reward()\n",
    "            # S1 = Categorical([self.term_state])\n",
    "            # return cross(S1, R)\n",
    "            yield (1, self.term_state, self.expected_term_reward(state))\n",
    "        else:\n",
    "            for r, p in state[action].to_discrete(self.quantization):\n",
    "                s1 = list(state[1])\n",
    "                s1[action] = r\n",
    "                yield (p, tuple(s1), self.cost)\n",
    "\n",
    "\n",
    "    def action_features(self, action, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        assert state is not None\n",
    "\n",
    "\n",
    "        if action == self.term_action:\n",
    "            return np.array([\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                self.expected_term_reward(state)\n",
    "            ])\n",
    "        else:\n",
    "#             gamble = action // self.outcomes\n",
    "            return np.array([\n",
    "                self.cost,\n",
    "                self.myopic_voi(action),\n",
    "                self.vpi_action(action),\n",
    "                self.vpi(),\n",
    "                self.expected_term_reward(state)\n",
    "            ])\n",
    "\n",
    "    def gamble_dists(self, state, sample_all = False):\n",
    "        sdist = state[0].sample_all() if sample_all else self.dist\n",
    "        grid = np.array(state[1]).reshape(self.gambles, self.outcomes)\n",
    "        return np.dot(grid, sdist)\n",
    "\n",
    "    @lru_cache(None)\n",
    "    def vpi(self):\n",
    "        sdist = self._state[0].sample_all() # make one big loop and sample each time \n",
    "        grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "        smus = [expectation(np.sum(sdist*grid[g])) for g in range(self.gambles)]\n",
    "        ssigmas = [np.sum(sdist*grid[g]).sigma for g in range(self.gambles)]\n",
    "        gambles = [Normal(smus[i],ssigmas[i])\n",
    "                   for i in range(self.gambles)]\n",
    "        samples_max = np.amax([[sample(gambles[i])\n",
    "                                for i in range(self.gambles)]\n",
    "                               for _ in range(2500)],1)\n",
    "        return np.mean(samples_max) - np.max(self.mus)\n",
    "    \n",
    "    @lru_cache(None)\n",
    "    def vpi2(self,state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        gambles = self.gamble_dists(state, sample_all = True)\n",
    "        samples_max = np.amax([[sample(gambles[i])\n",
    "                                for i in range(self.gambles)] for _ in range(2500)],1)\n",
    "        return np.mean(samples_max) - np.max(self.mus)\n",
    "\n",
    "    def grid(self,state=None):\n",
    "        if self._state is self.term_state:\n",
    "            return np.array(self.last_state).reshape(self.gambles,self.outcomes)\n",
    "        return np.array(self._state).reshape(self.gambles,self.outcomes)\n",
    "\n",
    "    @lru_cache(None)\n",
    "    def vpi_action(self, action):\n",
    "        #E[value if gamble corresponding to action is fully known]\n",
    "        if action >= self.attributes:\n",
    "            action -= self.attributes\n",
    "            gamble = action // self.outcomes\n",
    "            mus_wo_g = np.delete(self.mus,gamble)\n",
    "            k = np.max(mus_wo_g)\n",
    "            m = self.mus[gamble]\n",
    "            s = np.sqrt(self.vars[gamble])\n",
    "            e_higher = integrate.quad(lambda x: x*norm.pdf(x,m,s), k, np.inf)[0]\n",
    "            e_val = k*norm.cdf(k,m,s) + e_higher\n",
    "        else:\n",
    "            smaxmus = []\n",
    "            grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "            grid = np.copy(grid)\n",
    "            for _ in range(2500):\n",
    "                sdist = self._state[0].sample_all()\n",
    "#                 print(grid)\n",
    "                for j in range(self.gambles):\n",
    "                    grid[j,action] = sample(grid[j,action])\n",
    "#                 print(grid)\n",
    "                smus = [expectation(np.sum(sdist*grid[g])) for g in range(self.gambles)]\n",
    "                smaxmus.append(np.max(smus))\n",
    "            e_val = np.mean(smaxmus)\n",
    "        return e_val - np.max(self.mus)\n",
    "\n",
    "    #todo edit\n",
    "#     @lru_cache(None)\n",
    "    def myopic_voi(self, action):\n",
    "        #E[value if gamble corresponding to action is fully known]\n",
    "        if action >= self.attributes:\n",
    "            action -= self.attributes\n",
    "            gamble = action // self.outcomes\n",
    "            outcome = action % self.outcomes\n",
    "            mus_wo_g = np.delete(self.mus,gamble)\n",
    "            k = np.max(mus_wo_g)\n",
    "            m = self.mus[gamble]\n",
    "            s = self.reward.sigma*self.dist[outcome]\n",
    "            e_higher = integrate.quad(lambda x: x*norm.pdf(x,m,s), k, np.inf)[0]\n",
    "            e_val = k*norm.cdf(k,m,s) + e_higher\n",
    "        else:\n",
    "            smaxmus = []\n",
    "            grid = np.array(self._state[1]).reshape(self.gambles,self.outcomes)\n",
    "            for _ in range(2500):\n",
    "                sdist = self._state[0].sample_p(action, expectation = True)\n",
    "                smus = [expectation(np.sum(sdist*grid[g])) for g in range(self.gambles)]\n",
    "                smaxmus.append(np.max(smus))\n",
    "            e_val = np.mean(smaxmus)\n",
    "#         print(self.mus)\n",
    "        return e_val - np.max(self.mus)\n",
    "\n",
    "    def term_reward(self, state=None):\n",
    "        state = state[1] if state is not None else self._state[1]\n",
    "        grid = np.array(state).reshape(self.gambles,self.outcomes)\n",
    "        best_idx = np.argmax(self.mus)\n",
    "        return self.dist.dot(grid[best_idx])\n",
    "\n",
    "    def expected_term_reward(self, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        return max(map(expectation, self.gamble_dists(state)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
